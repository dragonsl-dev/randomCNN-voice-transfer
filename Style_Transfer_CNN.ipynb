{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Style Transfer CNN",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNJNmocrYz30iQD5mUV61Yy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dragonsl-dev/randomCNN-voice-transfer/blob/master/Style_Transfer_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srrYe7WevXYW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QswIdCSsvNIr"
      },
      "source": [
        "#### Recommended gpu: k80, P100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "go_M39b3vMpe"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMxPzK1voT2g"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRMk1wWliMIt",
        "outputId": "d9d1dda6-3c0f-447e-e0f2-80d35cd54451"
      },
      "source": [
        "!git clone https://github.com/dragonsl-dev/randomCNN-voice-transfer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'randomCNN-voice-transfer'...\n",
            "remote: Enumerating objects: 74, done.\u001b[K\n",
            "remote: Counting objects: 100% (74/74), done.\u001b[K\n",
            "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
            "remote: Total 125 (delta 38), reused 46 (delta 17), pack-reused 51\u001b[K\n",
            "Receiving objects: 100% (125/125), 7.55 MiB | 41.58 MiB/s, done.\n",
            "Resolving deltas: 100% (58/58), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQVY9j-UiNWQ",
        "outputId": "27bdbbf8-4d4a-4805-e529-a0dacdf1ec2d"
      },
      "source": [
        "!sudo apt install libav-tools"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Package libav-tools is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "However the following packages replace it:\n",
            "  ffmpeg\n",
            "\n",
            "E: Package 'libav-tools' has no installation candidate\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRMb3WNdiTE9",
        "outputId": "91ffe701-50f9-43bf-a54d-b586fbc5ad08"
      },
      "source": [
        "%cd randomCNN-voice-transfer/\r\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/randomCNN-voice-transfer\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (19.3.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.16.2)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.8.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (1.1.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (3.2.2)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r requirements.txt (line 2)) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r requirements.txt (line 2)) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r requirements.txt (line 2)) (2.5)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r requirements.txt (line 2)) (7.0.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 3)) (0.22.2.post1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 3)) (0.2.2)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 3)) (0.51.2)\n",
            "Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 3)) (0.10.3.post1)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 3)) (2.1.9)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 3)) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 3)) (1.19.5)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 3)) (4.4.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 4)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 4)) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 5)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 5)) (0.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pooch>=1.0->librosa->-r requirements.txt (line 3)) (20.9)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.6/dist-packages (from pooch>=1.0->librosa->-r requirements.txt (line 3)) (1.4.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pooch>=1.0->librosa->-r requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from resampy>=0.2.2->librosa->-r requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.43.0->librosa->-r requirements.txt (line 3)) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba>=0.43.0->librosa->-r requirements.txt (line 3)) (53.0.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile>=0.9.0->librosa->-r requirements.txt (line 3)) (1.14.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pooch>=1.0->librosa->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pooch>=1.0->librosa->-r requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pooch>=1.0->librosa->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pooch>=1.0->librosa->-r requirements.txt (line 3)) (2020.12.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa->-r requirements.txt (line 3)) (2.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpJ1-6gss1Mf"
      },
      "source": [
        "fix librosa issue"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "A1BpwW6Kr9OV",
        "outputId": "df31b24a-e770-4709-f103-3db6d3fb70be"
      },
      "source": [
        "#@title\n",
        "%%writefile utils.py\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "from model import *\n",
        "import soundfile as sf\n",
        "\n",
        "def wav2spectrum(filename):\n",
        "    x, sr = librosa.load(filename)\n",
        "    S = librosa.stft(x, N_FFT)\n",
        "    p = np.angle(S)\n",
        "\n",
        "    S = np.log1p(np.abs(S))\n",
        "    return S, sr\n",
        "\n",
        "\n",
        "def spectrum2wav(spectrum, sr, outfile):\n",
        "    # Return the all-zero vector with the same shape of `a_content`\n",
        "    a = np.exp(spectrum) - 1\n",
        "    p = 2 * np.pi * np.random.random_sample(spectrum.shape) - np.pi\n",
        "    for i in range(50):\n",
        "        S = a * np.exp(1j * p)\n",
        "        x = librosa.istft(S)\n",
        "        p = np.angle(librosa.stft(x, N_FFT))\n",
        "    \n",
        "    sf.write(outfile, x, sr, 'PCM_24')\n",
        "    #librosa.output.write_wav(outfile, x, sr)\n",
        "\n",
        "\n",
        "def wav2spectrum_keep_phase(filename):\n",
        "    x, sr = librosa.load(filename)\n",
        "    S = librosa.stft(x, N_FFT)\n",
        "    p = np.angle(S)\n",
        "\n",
        "    S = np.log1p(np.abs(S))\n",
        "    return S, p, sr\n",
        "\n",
        "\n",
        "def spectrum2wav_keep_phase(spectrum, p, sr, outfile):\n",
        "    # Return the all-zero vector with the same shape of `a_content`\n",
        "    a = np.exp(spectrum) - 1\n",
        "    for i in range(50):\n",
        "        S = a * np.exp(1j * p)\n",
        "        x = librosa.istft(S)\n",
        "        p = np.angle(librosa.stft(x, N_FFT))\n",
        "    #librosa.output.write_wav(outfile, x, sr)\n",
        "    sf.write(outfile, x, sr, 'PCM_24')\n",
        "\n",
        "def compute_content_loss(a_C, a_G):\n",
        "    \"\"\"\n",
        "    Compute the content cost\n",
        "    Arguments:\n",
        "    a_C -- tensor of dimension (1, n_C, n_H, n_W)\n",
        "    a_G -- tensor of dimension (1, n_C, n_H, n_W)\n",
        "    Returns:\n",
        "    J_content -- scalar that you compute using equation 1 above\n",
        "    \"\"\"\n",
        "    m, n_C, n_H, n_W = a_G.shape\n",
        "\n",
        "    # Reshape a_C and a_G to the (m * n_C, n_H * n_W)\n",
        "    a_C_unrolled = a_C.view(m * n_C, n_H * n_W)\n",
        "    a_G_unrolled = a_G.view(m * n_C, n_H * n_W)\n",
        "\n",
        "    # Compute the cost\n",
        "    J_content = 1.0 / (4 * m * n_C * n_H * n_W) * torch.sum((a_C_unrolled - a_G_unrolled) ** 2)\n",
        "\n",
        "    return J_content\n",
        "\n",
        "\n",
        "def gram(A):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    A -- matrix of shape (n_C, n_L)\n",
        "    Returns:\n",
        "    GA -- Gram matrix of shape (n_C, n_C)\n",
        "    \"\"\"\n",
        "    GA = torch.matmul(A, A.t())\n",
        "\n",
        "    return GA\n",
        "\n",
        "\n",
        "def gram_over_time_axis(A):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    A -- matrix of shape (1, n_C, n_H, n_W)\n",
        "    Returns:\n",
        "    GA -- Gram matrix of A along time axis, of shape (n_C, n_C)\n",
        "    \"\"\"\n",
        "    m, n_C, n_H, n_W = A.shape\n",
        "\n",
        "    # Reshape the matrix to the shape of (n_C, n_L)\n",
        "    # Reshape a_C and a_G to the (m * n_C, n_H * n_W)\n",
        "    A_unrolled = A.view(m * n_C * n_H, n_W)\n",
        "    GA = torch.matmul(A_unrolled, A_unrolled.t())\n",
        "\n",
        "    return GA\n",
        "\n",
        "\n",
        "def compute_layer_style_loss(a_S, a_G):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    a_S -- tensor of dimension (1, n_C, n_H, n_W)\n",
        "    a_G -- tensor of dimension (1, n_C, n_H, n_W)\n",
        "    Returns:\n",
        "    J_style_layer -- tensor representing a scalar style cost.\n",
        "    \"\"\"\n",
        "    m, n_C, n_H, n_W = a_G.shape\n",
        "\n",
        "    # Reshape the matrix to the shape of (n_C, n_L)\n",
        "    # Reshape a_C and a_G to the (m * n_C, n_H * n_W)\n",
        "\n",
        "    # Calculate the gram\n",
        "    # !!!!!! IMPORTANT !!!!! Here we compute the Gram along n_C,\n",
        "    # not along n_H * n_W. But is the result the same? No.\n",
        "    GS = gram_over_time_axis(a_S)\n",
        "    GG = gram_over_time_axis(a_G)\n",
        "\n",
        "    # Computing the loss\n",
        "    J_style_layer = 1.0 / (4 * (n_C ** 2) * (n_H * n_W)) * torch.sum((GS - GG) ** 2)\n",
        "\n",
        "    return J_style_layer\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "9VbAuCUUtxTj"
      },
      "source": [
        "#@title\n",
        "%%writefile train.py\n",
        "import matplotlib\n",
        "matplotlib.use('agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "from utils import *\n",
        "from model import *\n",
        "import time\n",
        "import math\n",
        "import argparse\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('-content', help='Content input')\n",
        "parser.add_argument('-content_weight', help='Content weight. Default is 1e2', default = 1e2)\n",
        "parser.add_argument('-style', help='Style input')\n",
        "parser.add_argument('-style_weight', help='Style weight. Default is 1', default = 1)\n",
        "parser.add_argument('-epochs', type=int, help='Number of epoch iterations. Default is 20000', default = 20000)\n",
        "parser.add_argument('-print_interval', type=int, help='Number of epoch iterations between printing losses', default = 1000)\n",
        "parser.add_argument('-plot_interval', type=int, help='Number of epoch iterations between plot points', default = 1000)\n",
        "parser.add_argument('-learning_rate', type=float, default = 0.002)\n",
        "parser.add_argument('-output', help='Output file name. Default is \"output\"', default = 'output')\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "CONTENT_FILENAME = args.content\n",
        "STYLE_FILENAME = args.style\n",
        "\n",
        "a_content, sr = wav2spectrum(CONTENT_FILENAME)\n",
        "a_style, sr = wav2spectrum(STYLE_FILENAME)\n",
        "\n",
        "a_content_torch = torch.from_numpy(a_content)[None, None, :, :]\n",
        "if cuda:\n",
        "    a_content_torch = a_content_torch.cuda()\n",
        "print(a_content_torch.shape)\n",
        "a_style_torch = torch.from_numpy(a_style)[None, None, :, :]\n",
        "if cuda:\n",
        "    a_style_torch = a_style_torch.cuda()\n",
        "print(a_style_torch.shape)\n",
        "\n",
        "model = RandomCNN()\n",
        "model.eval()\n",
        "\n",
        "a_C_var = Variable(a_content_torch, requires_grad=False).float()\n",
        "a_S_var = Variable(a_style_torch, requires_grad=False).float()\n",
        "if cuda:\n",
        "    model = model.cuda()\n",
        "    a_C_var = a_C_var.cuda()\n",
        "    a_S_var = a_S_var.cuda()\n",
        "\n",
        "a_C = model(a_C_var)\n",
        "a_S = model(a_S_var)\n",
        "\n",
        "\n",
        "# Optimizer\n",
        "learning_rate = args.learning_rate\n",
        "a_G_var = Variable(torch.randn(a_content_torch.shape) * 1e-3)\n",
        "if cuda:\n",
        "    a_G_var = a_G_var.cuda()\n",
        "a_G_var.requires_grad = True\n",
        "optimizer = torch.optim.Adam([a_G_var])\n",
        "\n",
        "# coefficient of content and style\n",
        "style_param = args.style_weight\n",
        "content_param = args.content_weight\n",
        "\n",
        "num_epochs = args.epochs\n",
        "print_every = args.print_interval\n",
        "plot_every = args.plot_interval\n",
        "\n",
        "# Keep track of losses for plotting\n",
        "current_loss = 0\n",
        "all_losses = []\n",
        "\n",
        "\n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "# Train the Model\n",
        "\n",
        "try:\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        optimizer.zero_grad()\n",
        "        a_G = model(a_G_var)\n",
        "\n",
        "        content_loss = content_param * compute_content_loss(a_C, a_G)\n",
        "        style_loss = style_param * compute_layer_style_loss(a_S, a_G)\n",
        "        loss = content_loss + style_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print\n",
        "        if epoch % print_every == 0:\n",
        "            print(\"{} {}% {} content_loss:{:4f} style_loss:{:4f} total_loss:{:4f}\".format(epoch,\n",
        "                                                                                          epoch / num_epochs * 100,\n",
        "                                                                                          timeSince(start),\n",
        "                                                                                          content_loss.item(),\n",
        "                                                                                          style_loss.item(), loss.item()))\n",
        "            current_loss += loss.item()\n",
        "\n",
        "        # Add current loss avg to list of losses\n",
        "        if epoch % plot_every == 0:\n",
        "            all_losses.append(current_loss / plot_every)\n",
        "            current_loss = 0\n",
        "except KeyboardInterrupt:\n",
        "    print(\"User interrupted training\")\n",
        "\n",
        "gen_spectrum = a_G_var.cpu().data.numpy().squeeze()\n",
        "gen_audio_C = args.output + \".wav\"\n",
        "spectrum2wav(gen_spectrum, sr, gen_audio_C)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)\n",
        "plt.savefig('loss_curve.png')\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "# we then use the 2nd column.\n",
        "plt.subplot(1, 1, 1)\n",
        "plt.title(\"Content Spectrum\")\n",
        "plt.imsave('Content_Spectrum.png', a_content[:400, :])\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "# we then use the 2nd column.\n",
        "plt.subplot(1, 1, 1)\n",
        "plt.title(\"Style Spectrum\")\n",
        "plt.imsave('Style_Spectrum.png', a_style[:400, :])\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "# we then use the 2nd column.\n",
        "plt.subplot(1, 1, 1)\n",
        "plt.title(\"CNN Voice Transfer Result\")\n",
        "plt.imsave('Gen_Spectrum.png', gen_spectrum[:400, :])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLxZ4ULVoWMc"
      },
      "source": [
        "# Infer\r\n",
        "Upload content and style to input directory\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "```\r\n",
        "-content input/orig.wav    # voice to modify\r\n",
        "-style input/twi.wav       # song / source\r\n",
        "```\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlAduiNDuLWO",
        "outputId": "d1262be5-93d3-46b6-f0ed-0faa1316a17b"
      },
      "source": [
        "# download samples\r\n",
        "!wget https://github.com/dragonsl-dev/wav-samples/raw/main/orig.wav -P input/\r\n",
        "!wget https://github.com/dragonsl-dev/wav-samples/raw/main/twi2.wav -P input/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-05 15:07:16--  https://github.com/dragonsl-dev/wav-samples/raw/main/orig.wav\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/dragonsl-dev/wav-samples/main/orig.wav [following]\n",
            "--2021-02-05 15:07:16--  https://raw.githubusercontent.com/dragonsl-dev/wav-samples/main/orig.wav\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2428750 (2.3M) [audio/wav]\n",
            "Saving to: ‘input/orig.wav’\n",
            "\n",
            "orig.wav            100%[===================>]   2.32M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-02-05 15:07:16 (49.8 MB/s) - ‘input/orig.wav’ saved [2428750/2428750]\n",
            "\n",
            "--2021-02-05 15:07:16--  https://github.com/dragonsl-dev/wav-samples/raw/main/twi2.wav\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/dragonsl-dev/wav-samples/main/twi2.wav [following]\n",
            "--2021-02-05 15:07:17--  https://raw.githubusercontent.com/dragonsl-dev/wav-samples/main/twi2.wav\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 720940 (704K) [audio/wav]\n",
            "Saving to: ‘input/twi2.wav’\n",
            "\n",
            "twi2.wav            100%[===================>] 704.04K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-02-05 15:07:17 (42.5 MB/s) - ‘input/twi2.wav’ saved [720940/720940]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qg49iJvMyZYs"
      },
      "source": [
        "# upload your own to input directory in the sidebar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38nQlxyriZEi",
        "outputId": "bef0c302-22ae-4b31-f7a6-ab9e859aa68e"
      },
      "source": [
        "\r\n",
        "!python train.py -epochs 200000 -print_interval 50 -content input/orig.wav -style input/twi2.wav"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1, 257, 1090])\n",
            "torch.Size([1, 1, 257, 705])\n",
            "50 0.025% 0m 4s content_loss:3.837719 style_loss:1594.863403 total_loss:1598.701172\n",
            "100 0.05% 0m 13s content_loss:3.256724 style_loss:1333.706055 total_loss:1336.962769\n",
            "150 0.075% 0m 21s content_loss:3.004975 style_loss:1009.639526 total_loss:1012.644531\n",
            "200 0.1% 0m 29s content_loss:2.869258 style_loss:748.779358 total_loss:751.648621\n",
            "250 0.125% 0m 38s content_loss:2.746639 style_loss:545.045471 total_loss:547.792114\n",
            "300 0.15% 0m 47s content_loss:2.671658 style_loss:392.923981 total_loss:395.595642\n",
            "350 0.17500000000000002% 0m 57s content_loss:2.639799 style_loss:286.733704 total_loss:289.373505\n",
            "400 0.2% 1m 6s content_loss:2.634223 style_loss:215.552063 total_loss:218.186279\n",
            "450 0.22499999999999998% 1m 16s content_loss:2.637390 style_loss:168.807190 total_loss:171.444580\n",
            "500 0.25% 1m 26s content_loss:2.639517 style_loss:138.481857 total_loss:141.121368\n",
            "550 0.27499999999999997% 1m 36s content_loss:2.633878 style_loss:118.981483 total_loss:121.615364\n",
            "600 0.3% 1m 46s content_loss:2.614728 style_loss:106.255859 total_loss:108.870590\n",
            "650 0.325% 1m 56s content_loss:2.578526 style_loss:97.061546 total_loss:99.640076\n",
            "700 0.35000000000000003% 2m 6s content_loss:2.529405 style_loss:88.395638 total_loss:90.925041\n",
            "750 0.375% 2m 16s content_loss:2.485386 style_loss:77.876915 total_loss:80.362305\n",
            "800 0.4% 2m 25s content_loss:2.470011 style_loss:65.942085 total_loss:68.412094\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ij0UpUNpPNm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}